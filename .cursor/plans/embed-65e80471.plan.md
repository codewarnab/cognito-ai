<!-- 65e80471-fd98-4214-8d00-677fb54a7349 f3add66a-3b27-45a8-8ce4-2b3169beefcc -->
# `src/workers/embed-worker.ts` — Embedding Worker Plan

### Runtime & Model

- Use ONNX Runtime Web with WebGPU + IOBinding for max throughput; fallback to WASM (SIMD+threads).
- Tokenizer via `@xenova/transformers` tokenizer-only load (no model) to avoid custom WordPiece logic; zero network by loading assets from Cache Storage.
- Model: quantized `all-MiniLM-L6-v2` ONNX (e.g., int8 weights, fp16/32 activations) stored in Cache Storage under `model/<version>/...`.

### Asset Loading (Zero-Network)

- Resolve all assets via Cache Storage only; if missing, respond `INIT_ERROR: MISSING_ASSETS`.
- Use `caches.open('model') → cache.match(url)` and feed `Response.body`/`ArrayBuffer` into ORT session creation and tokenizer init.
- Guard every op behind `state.modelReady && state.tokenizerReady`.

### Message API (worker <→ offscreen bridge)

- `INIT { modelVersion } → { ok, runtime: 'webgpu'|'wasm', deviceLimits, batchSize }`
- `CHUNK_TEXT { url, text, window=240, stride=200 } → { chunks: string[], meta }`
- `EMBED_TEXTS { ids: string[], texts: string[], batchSize? } → { ok, embedded: {id, l2norm:boolean}[], failed: {id, error}[] }`
- `INDEX_CHUNKS { items: [{chunkId, url, chunkIndex, text}] } → { ok, upserted, failed[] }` (embeds+persists)
- `EMBED_QUERY { text } → { vector: Float32Array }`
- `SEARCH_BRUTE { vector, topK=20 } → { hits: [{chunkId, url, score}] }`
- `FLUSH → { ok }` (ensure pending writes committed)
- `STATS → { gpuMemMB, wasmHeapMB, queuedJobs, indexedChunks }`
- `SHUTDOWN → { ok }`
- Errors respond with `{ ok:false, code, message }` and preserve partial results where applicable.

### Chunking

- Tokenizer-aware windowing: 240 tokens per window, stride 200 (40 overlap).
- Fallback to word-based 200± tokens if tokenizer unavailable (shouldn’t happen post-INIT).
- Emit per-chunk metadata: `{chunkIndex, tokenLength, startToken}`.

### Embedding Pipeline

- Dynamic batch size target 8–16; auto-tune by probing first 1–2 runs to fit GPU/heap budget.
- Mean-pool token embeddings (exclude padding) → L2 normalize per vector.
- Pre-allocate input/output `GPUBuffer`/TypedArrays; reuse across batches via IOBinding.
- Backpressure-aware FIFO job queue; concurrency=1 to maximize GPU occupancy and avoid thrash.

### Storage (IndexedDB)

- Write vectors as `Float32Array.buffer` (little-endian) in `chunks` store: `{ chunkId, url, chunkIndex, tokenLength, textSnippet, embedding(ArrayBuffer) }`.
- Use `idb` helper; bulk `put` in a single transaction per batch; commit every ≤10 inserts.
- Cap per page to 50 chunks; on overflow, evict oldest chunkIds of that `url`.

### Search (Dense-only; <10k)

- Brute-force cosine similarity:
  - Read vectors as `Float32Array` views without copying.
  - Query vector must be L2-normalized; use dot product as cosine.
  - Early-out if `indexedChunks` ≥ 10k triggers optional ANN path (stub hook).

### Backpressure & Flow Control

- Maintain `pendingBytes` and `pendingJobs`. If `pendingBytes > 64MB` (WASM) or `gpuPending > 256MB` (WebGPU), respond `BUSY {retryInMs}`.
- Yield to event loop between batches (`await nextMicrotask`) to keep SW responsive.
- Cancel tokens (optional): `EMBED_TEXTS` can be sent with `jobId` and later `CANCEL { jobId }`.

### Error Handling & Partial Failures

- All multi-item ops are best-effort: continue batch on per-item failure; collect `{id, error}`.
- Classify errors: `MISSING_ASSETS`, `RUNTIME_UNAVAILABLE`, `OOM`, `BAD_INPUT`, `TX_FAIL`.
- Automatic recovery:
  - On WebGPU device loss: try one reinit (once per session), then degrade to WASM.
  - On `OOM`: halve batch size and retry once.
  - On IndexedDB tx error: retry with smaller batch (e.g., 5) once.

### Memory/Perf Budgets

- WebGPU: aim ≤ 512 MB total allocations; keep per-batch activation/output ≤ 128 MB.
- WASM: heap growth ≤ 256 MB; batch size shrinks to fit.
- Tokenizer cache: LRU up to 8k tokens.
- Reuse buffers; avoid cloning vectors; pass `ArrayBuffer` with `postMessage(..., [buffer])` when crossing threads.

### Acceptance Benchmarks

- Initialization: tokenizer+model ready ≤ 2.0s p50 (WebGPU), ≤ 4.0s p95; WASM ≤ 6.0s p95.
- Throughput (WebGPU): ≥ 1,000 chunks/min with 240-token windows, batch 12 on typical laptop iGPU.
- Batch latency (WebGPU): ≤ 120 ms p50, ≤ 250 ms p95 for 12×240 tokens.
- Search (<10k): query embed ≤ 40 ms p50; brute-force topK 20 in ≤ 30 ms p50.
- Accuracy: cosine to reference CPU within 1e-4; L2 norm within 1e-6.

### Essential Interfaces (illustrative)

```typescript
// message union (simplified)
type Msg =
  | { type: 'INIT'; modelVersion: string }
  | { type: 'CHUNK_TEXT'; url: string; text: string; window?: number; stride?: number }
  | { type: 'EMBED_TEXTS'; ids: string[]; texts: string[]; batchSize?: number; jobId?: string }
  | { type: 'INDEX_CHUNKS'; items: { chunkId: string; url: string; chunkIndex: number; text: string }[] }
  | { type: 'EMBED_QUERY'; text: string }
  | { type: 'SEARCH_BRUTE'; vector: Float32Array; topK?: number }
  | { type: 'FLUSH' } | { type: 'STATS' } | { type: 'SHUTDOWN' };

// cosine over typed arrays
function topKCosine(query: Float32Array, corpus: Iterable<{ id: string; vec: Float32Array }>, topK: number) {
  // returns sorted [{id, score}] without copying vectors
}
```

### Future-proofing (ANN)

- Optional HNSW WASM module behind feature flag once `indexedChunks ≥ 10k`.
- Keep an adapter layer: `searchProvider = bruteForce | hnsw` with identical interface.

### Telemetry (local-only)

- Track `p50/p95` for init, batch latency, search time; retain rolling window in `settings` store.
- No network egress; expose via `STATS`.

### Integration Notes

- Offscreen bridge ensures worker lifetime; postMessage transfers large buffers.
- Background should not call `EMBED_*` until `INIT.ok === true` and runtime selected.
- Respect `paused` setting: worker ignores new `INDEX_*` jobs while paused, but allows `SEARCH_*`.

### To-dos

- [ ] Wire ONNX Runtime Web (WebGPU + WASM fallback) and tokenizer-only init
- [ ] Load model/tokenizer exclusively from Cache Storage with guards
- [ ] Implement worker message API with structured results and errors
- [ ] Add tokenizer-aware chunking (window 240, stride 200) with fallback
- [ ] Batch embed (8–16), mean-pool, L2 normalize, buffer reuse, IOBinding
- [ ] Persist Float32 embeddings per chunk in IndexedDB with caps
- [ ] Implement typed-array cosine search and topK selection
- [ ] Add queue, budgets, BUSY responses, and cancellation
- [ ] Map and recover from OOM, device loss, tx errors; partial results
- [ ] Measure p50/p95 init, throughput, latency; expose via STATS